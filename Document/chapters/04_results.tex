% !TeX root = ../main.tex

\chapter{Evaluation}\label{chapter:Evaluation}

In this chapter we will first show what our methodology for experimentation was, the reasoning behind each experiment, their value and contribution towards this thesis's goal, and how we went about doing each one. Secondly, we will show, and analyze all the results of said experiments.

\section{Experiments}

The experiments shown in this thesis have been done using our own developed software, shown in chapter 3, and the specifications of the computer used to develop, run, and maintain the software are:

\begin{itemize}
	\item OS: Ubuntu 16.04
	\item CPU: Intel i7 4790k @ 4.6Ghz
	\item RAM: 16GB 1600Mhz
	\item GPU: NVIDIA GTX 1080
\end{itemize}

And though the specifications of this computer are well above what was needed to both, develop and run our experiments, we can say with certainty that any computer with similar specifications can replicate anything shown in this chapter with ease.

\subsection{Docker container setup}

We have mentioned previously the use of MACKE, but never touched on the topic of its setup. MACKE being a wrapper around KLEE, requires a full installation of first, a specific version of KLEE, which in and of itself can be a lengthy process. Furthermore, some programs need to be installed in a specific order -- not all -- and if a step is omitted or disregarded, KLEE and MACKE are sure to not run properly, be erratic, or flat-out not work.

Seeing this as a problem, and knowing that our experiments needed the utmos precision, and sometimes required a bit of experimentation with some of the programs involved, we knew we had to create an environment that could be shared among us, that was confirmed to be working fine, that was disposable, yet upgradable. To this end, we developed a Docker \parencite{docker} image, which has MACKE completely configured, and ready to be executed. The image is also disposable, and can be run and discarded as many times as needed. The image can be downloaded by doing:

\begin{lstlisting}[language=bash]
$ docker pull ricardonales/macke:0.6
\end{lstlisting}

And to run the image, and work with it:

\begin{lstlisting}[language=bash]
$ docker run -ti ricardonales/macke:0.6 bash
\end{lstlisting}

Then we need to start Python virtual environment where MACKE is available, and in order to do this we need to run the following command (it is worth noting that the container only has the root user, so the next command needs to be written "as is"):

\begin{lstlisting}[language=bash]
# source ~/build/macke/.venv/bin/activate
\end{lstlisting}

After doing this MACKE will be available by running "macke" as shown in Figure 4.1 .

\begin{figure}[H]
	\caption{MACKE running on container}
	\centering
	\includegraphics[width=1.0\textwidth]{macke_container}
\end{figure}


\subsection{Pre-processing the data}

In order to be able to run programs through MACKE we require to have their bitcode files generated by compiling them with LLVM, in order to do this, using our pre-configured container with all tools needed, we need to run the programs through "make+llvm", to do so we did the following:
\\

Enter the MACKE container and run the following command to enter the Python virtual environment containing make+llvm:

\begin{lstlisting}[language=bash]
# source ~/build/MakeAdditions/.venv/bin/activate
\end{lstlisting}

This will allow the program make+llvm \parencite{thomasThesis} . The next step needed is to go to our program and run its "configure" file, so that it generates its Makefile, which is needed by make+llvm.

The next thing we did was run make+llvm on the Makefile by running:

\begin{lstlisting}[language=bash]
# make+llvm Makefile
\end{lstlisting}

This process might take some time, since it will first build the required files from the Makefile, and then immediately run llvm on it. This process will generate, if successful, all binary files needed.

Alternatively, one can run LLVM directly on a specific $c$ or $cpp$ file by doing the following:

\begin{lstlisting}[language=bash]
# clang -I <LIBRARIES_FOLDER> -emit-llvm -c -g <FILE>.c
\end{lstlisting}

We point out that <LIBRARIES\_FOLDER> must include the path (relative or absolute). By doing this, we can also generate the bitcode of a specific file.

This is al the pre-processing needed in order to be able to work with the the programs. It is extremely important to point out that not all programs will compile with make+llvm, and that sometimes there will be the need to isolate files, and run clang or clang++ directly on them.

\subsection{Runnig MACKE on programs}

Once all the bitcode files have been extracted for all programs, we are able to run MACKE on them, and with the container already pre-configured and with a proper MACKE installation, we can say with certainty that running the following commands will lead to the desired outcome:

\begin{lstlisting}[language=bash]
# macke <PROGRAM>.bc
\end{lstlisting}

With that, MACKE will run over the program, and run the composite symbolic execution, which can take up to hours depending on how big the program analyzed is. The result of running MACKE over rzip.bc is shown in Figure 4.2 .

\begin{figure}[H]
	\caption{Running MACKE on rzip}
	\centering
	\includegraphics[width=1.0\textwidth]{macke_on_rzip}
\end{figure}

As we can see MACKE found 26 vulnerabilities spread across 6 functions. To see this data one must go to the results folder, which is found under "/tmp/macke/macke-last", or if it is not the latest run of MACKE, it will be under "/tmp/macke/DATETIME\_WHEN\_RUN". The file that has the results information is klee.json, which we can read with either with a simple :

\begin{lstlisting}[language=bash]
# cat klee.json
\end{lstlisting}

Or by running:

\begin{lstlisting}[language=bash]
# jq . klee.json
\end{lstlisting}

We used this last option since the result is much more readable, and structured. The results of rzip can be seen in Figure 4.3 .

\begin{figure}[H]
	\caption{MACKE rzip's results}
	\centering
	\includegraphics[width=0.9\textwidth]{macke_rzip_results}
\end{figure}

\subsection{CVSS score assignment}

Using the NVD database to use reliable data, that met the criteria detailed in chapter 3, we created a compilation of the CVSS3 scores of the functions with vulnerabilities. To do this, we first searched through the NVD provided JSON files which include all vulnerabilities found in a structured way. In said files we searched for the "CVE Dictionary Entry" as NVD calls it \parencite{nvd} and compiled all of them in a new structure, which can be seen in Figure 4.4 .

\begin{figure}[H]
	\caption{CVSS3 Scores File}
	\centering
	\includegraphics[width=0.9\textwidth]{cvss_file}
\end{figure}

This scores would allow us to map them to the vulnerabilities found through MACKE. There was as problem with this solution though: our pool of base scores was too low, 24 to be precise.

Knowing this we set out to manually extract the base scores of the rest of vulnerabilities that were buffer overflow related. This procedure was lengthy and not straightforward, and showed us how subjective assessing some of the base scores can be. Furthermore, by looking at the procedure manually we were able to see that there were patterns, yet they could not be proven until we ran tests with the node attributes we had selected as the impact factors of the CVSS3 scores.

We were able to collect 23 more sampls, which in total allowed us to get a total of 47 CVSS3 scores which we were certain had been thoroughly analyzed and could be reliably used for learning purposes without having noisy data.

\subsection{Getting Call Graphs and Node Attributes}

The data that we had from all programs, after running LLVM on them was a bitcode file, but with this we still needed to extract the dot -- which has the call graph information -- from the bitcode. This has already been thoroughly explained in chapter 3.

Furthermore, and as mentioned as well in the aforementioned chapter, we were able to remove all redundant edges from the original callgraph, therefore allowing us to make a proper analysis of the call graphs.

The pre-processing of the data was mainly running all programs through our Python node\_attributes program, which generates a file that has the same name of the source dot file, with "\_node\_attributes.json" appended to it. This file contains all nodes, with their respective node attributes. In the case that CVSS3 scores were available from NVD, or the manually extracted CVSS3 --explained to detail in the next subsection -- scores we extracted. This is shown in Figure  4.3 .

\begin{figure}[H]
	\caption{Pre-processed Data}
	\centering
	\includegraphics[width=1.0\textwidth]{preprocessed_data}
\end{figure}

This file does not contain callgraph data, it is only the node attributes data, and if we had available CVSS3 data, from our compilation of base scores.

\subsection{Learning Setup}

We have shown both parts needed in order for us to be able to use a learning algorithm:

\begin{enumerate}
	\item The $X$ is the node attributes found on a per node basis.
	\item We have several $y$, one for every CVSS3 base score. 
\end{enumerate}

In order to implement, and test different learning algorithms we chose to use SciKit \parencite{scikit}, which is one of the most widely used Python libraries in the world for machine learning. With this library we would be able to test two different classifier algorithms, and see how they behaved.

\subsubsection{Random Forest and Naive Bayes}

We decided to use these two classifiers, since both have been shown to be robust classifiers, though we had to compare their theoretical efficiency. Following is a list of pros/cons of each classifier.
\\\\
Random Forests:
\begin{itemize}
	\item + It is robust against overfitting
	\item + It gives better results with the increasing number of examples
	\item + It might be used for clustering, statistical inference and feature selection as well
	\item + Works good with numerical, categorical data
	\item - Slower to train
	\item - Need to be set well its randomization parameters. (Selection of nodes, number of trees, randomization of instance variables)
	\item - Some objections for it overfitting
\end{itemize}
Naive Bayes:
\begin{itemize}
	\item + Easy to trains and understand the results
	\item + It has different extensions for different needs
	\item + Its model is smaller than the random forests since you need to keep all the trees in memory
	\item + Promising results for textual tasks
	\item - Based on naive assumptions that re not generally concordant with the data
	\item - It is really fragile to overfitting without any regularization assumption
\end{itemize}

These benefits and drawbacks of each one give us an outlook of how results will look, though only through experimentation would we be able to see if these applied to our dataset.

\subsubsection{Split dataset}

\todo{Talk here about 4 folds (KFold in the code)}

\subsubsection{Cross validation}

\todo{Talk about best model selection here.}

\subsection{Front-end implementation}

One of the crusial parts of our thesis is the front-end, since it is the point where users can finally interact with everything else that has been developed. Right from the begining we knew our framework would need to fulfil the following requirements:

\begin{enumerate}
	\item Be a web application that can be accessed remotely
	\item Have a simple user interface, and good user experience
	\item Implement a way for users to let us know how we could improve
\end{enumerate}

What we decided to develop was a Client-Server application that uses HTTPS as a secure link between the two when passing requests and responses. The application stack looks as follows:

\begin{itemize}
	\item Back-end: NodeJS using Express as the server API 
	\item Front-end: ReactJS using server side rendering
\end{itemize}

What we mean by server side rendering, is that NodeJS being able to render JS code, it allows for faster loading times, since the browser does not need to render the initially received JS code, since it has already been rendered to HTML. This speeds-up the process in slow computers, and also makes the backend structure more robust, since we are able to fine-grain handle requests, instead of just sending the entire ReactJS bundle along with a dummy HTML page. All this code can be analyzed under \parencite{ricardo}, and follows the normal structure of SSR ReactJS applications.

\begin{figure}[H]
	\caption{Front-end tutorial page}
	\centering
	\includegraphics[width=1.0\textwidth]{explanation_frontend}
\end{figure}

First we added a "Tutorial" page where we explain the user what they are about to do, what they can expect, and what we are expecting of them as well. This can be see in Figure 4.6 .

Second we show the users what they should do in a step by step manner. This part is very thorough so that in case a user happens to get stuck, they have always these instructions to fall back on. This can be see in Figure 4.7 .

\begin{figure}[H]
	\caption{Front-end instructions}
	\centering
	\includegraphics[width=0.6\textwidth]{instructions}
\end{figure}

When the user clicks either on the navigation bar on "Program Selection" or on the green button under our instructions "Select Program" they are taken to the next step, which consists on a selection of the program that they would like to analyze out of all the programs mentioned in 3.3.1 . 

\begin{figure}[H]
	\caption{Front-end program selection}
	\centering
	\includegraphics[width=1.0\textwidth]{program_selection}
\end{figure}

Once the user clicks on "View Callgraph" he will be shown the view shown in Figure 4.9 . It includes the Callgraph on the top part, which is fully interactable, a reference on the color coding used on nodes depending on ther CVSS3 severity \parencite{cvss3}. If the user clicks on "Code" a menu is expanded that allows the download of the source code used to get the Callgraph shown. The "Node Attributes" button is also collapsed and needs to be clicked on in order to expand all node attributes of the currently selected node. The bottom part shows the CVSS3 base scores if they are available. It works in the form of a calculator, identical to official CVSS 3.0 calculator found at \parencite{cvss3}, and will show the new severity score as soon as a base score changes. 

\begin{figure}[H]
	\caption{Front-end Callgraph and CVSS3 Scores}
	\centering
	\includegraphics[width=1.0\textwidth]{callgraph_and_cvss}
\end{figure}

By having all of these features we fulfilled what we set out to do in terms of requirements, though we know there is room for improvement, and that we require feedback from professionals, which leads us to the Survey.

\subsection{Survey}

The target group of our framework is professionals in the area of IT Security with experience in C or C++, who could assess the vulnerabilities with the use of CVSS3 manually. There needs to be a way for said professionals to let us know what they think of the results provided by our framework.

Our front-end implementation includes a Survey that allows user's to send us their opinion about the framework. A screenshot of the Survey can be seen in Figure 4.6

\begin{figure}[H]
	\caption{Survey}
	\centering
	\includegraphics[width=1.0\textwidth]{survey}
\end{figure}

As shown, we leave the user's personal information as optional, and focus mainly on their opinion. We incentivize the user to give us their personal information in case we need to get back to them and ask them for further feedback and/or advice on how to improve upon what has already been developed.

\subsection{Including Feedback}

All of the feedback is invaluable to us, the reasoning behind this has to do with the nature of vulnerabilities and CVSS itself. When assessing them, as shown in section 4.1.4 , it can sometimes be hard to come up with a score that is not only accurate, but free of personal bias. To this end, we have added the previously mentioned Survey. All feedback received will be added as future work for this project, and will shape the project in the long term. Depending completely on the data received from the survey will we know how good our model fares when predicting the base scores of vulnerabilities found through symbolic execution, and if the impact factors selected such as the node attributes either need to be changed or improved.

\section{Results}

In the previous section we have shown all the requirements, and the preparation of the tools needed for our experimentation. In this section we will show all the results and draw conclusions regarding what the experimental data shown against our hypothesis, and previous assumptions. 

\subsection{MACKE vulnerabilities found}

After running MACKE on all 21 programs listed under section 3.3.1, we were able to find a wide variety of vulnerabilities in across all programs. In the following table we list the results for each program:

 \begin{table}[H]
 	\centering
 	\caption{MACKE vulnerabilities found}
 	\begin{tabular}{ |p{4cm}||p{9cm}|  }
 		\hline
 		Program & Functions with vulnerabilities\\
 		\hline
 		BlueZ 5.42   & p\_indent, parse, process\_frames, read\_n, read\_dump, read\_dump \\
 		AutoTrace 0.31.1 &   Vulnerabilities \\
 		GraphicsMagic 1.3.25 & Vulnerabilities \\
 		Icoutils 0.31.1    & Vulnerabilities \\
 		ImageMagic 6.0.4-8    & Vulnerabilities \\
 		Jasper 1.900.27    & Vulnerabilities \\
 		Jasper 2.0.10    & Vulnerabilities \\
 		Libsarchive 3.2.1	& Vulnerabilities \\
 		Libass 0.13.3	& Vulnerabilities \\
 		Libmad 0.15.1	& Vulnerabilities \\
 		Libplist 1.12	& Vulnerabilities \\
 		Libdsndfile 1.0.28	& Vulnerabilities \\
 		Libxml2 2.9.4	& Vulnerabilities \\
 		Lrzip 0.631	& Vulnerabilities \\
 		Openslp 2.0.0	& Vulnerabilities \\
 		Potrace 1.12	& Vulnerabilities \\
 		Rzip 2.1	& Vulnerabilities \\
 		Tcpdump 4.9.0	& Vulnerabilities \\
 		Tif Firread	& Vulnerabilities \\
 		Virglrenderer 0.5.0	& Vulnerabilities \\
 		Ytnef 1.9.2	& Vulnerabilities \\
 		\hline
 	\end{tabular}
 \end{table}
\todo{Add data}

It is clear that a tool such as MACKE using compositional symbolic execution is a reliable way of finding vulnerabilities. It was able to find a wide variety of vulnerabilities, and though we only focused on buffer-overflow related vulnerabilities, we still had plenty of data to analyze from these results. The most important thing was to see whether or not the vulnerabilities found in the NVD database were also found by MACKE.

\subsubsection{Common vulnerabilities found on NVD}

We have listed all vulnerabilities found already, but it is crucial to put them into perspective, and compare them with the vulnerabilities found in the NVD database. To that end, we will list the vulnerabilities found in the NVD database, the functions with the documented vulnerabilities and then the percentage of them that were found by MACKE.

 \begin{table}[H]
	\centering
	\caption{NVD vulnerabilities found by MACKE}
	\begin{tabular}{ |p{4cm}||p{5cm}||p{4cm}|  }
		\hline
		Program & Functions with NVD vulnerabilities & Percentage found by MACKE\\
		\hline
		BlueZ 5.42   & p\_indent, parse, process\_frames, read\_n, read\_dump, read\_dump & 100\% \\ 
		AutoTrace 0.31.1 &   Vulnerabilities & 100\% \\
		GraphicsMagic 1.3.25 & Vulnerabilities & 100\% \\
		Icoutils 0.31.1    & Vulnerabilities & 100\% \\
		ImageMagic 6.0.4-8    & Vulnerabilities & 100\% \\
		Jasper 1.900.27    & Vulnerabilities & 100\% \\
		Jasper 2.0.10    & Vulnerabilities & 100\% \\
		Libsarchive 3.2.1	& Vulnerabilities & 100\% \\
		Libass 0.13.3	& Vulnerabilities & 100\% \\
		Libmad 0.15.1	& Vulnerabilities & 100\% \\
		Libplist 1.12	& Vulnerabilities & 100\% \\
		Libdsndfile 1.0.28	& Vulnerabilities & 100\% \\
		Libxml2 2.9.4	& Vulnerabilities & 100\% \\
		Lrzip 0.631	& Vulnerabilities & 100\% \\
		Openslp 2.0.0	& Vulnerabilities & 100\% \\
		Potrace 1.12	& Vulnerabilities & 100\% \\
		Rzip 2.1	& Vulnerabilities & 100\% \\
		Tcpdump 4.9.0	& Vulnerabilities & 100\% \\
		Tif Firread	& Vulnerabilities & 100\% \\
		Virglrenderer 0.5.0	& Vulnerabilities & 100\% \\
		Ytnef 1.9.2	& Vulnerabilities & 100\% \\
		\hline
	\end{tabular}
\end{table}
\todo{Add data}

After taking a look at these results it is clear that MACKE had an incredible amount of coverage, not only finding all vulnerabilities documented on the NVD database, but also finding a good amount of unique ones. Furthermore, these results gave us some certainty that our procedure was headed in the right direction, since we relied heaviliy in havin a high percentage of vulnerabilities from the NVD database also be found by MACKE.

\subsubsection{Unique undocumented vulnerabilities found}

\todo{HERE}

\subsection{Learning results}

\todo{HERE}

\subsection{Survey Results}

\todo{HERE}

\subsection{Future work}

\todo{HERE}